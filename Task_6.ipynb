{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тестовое множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации документа по категориям, посчитать точность на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from pymorphy2 import tokenizers\n",
    "import spacy\n",
    "import nltk\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1  \\\n",
       "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1  culture    Рекордно дорогую статую майя признали подделкой   \n",
       "\n",
       "                                                   2  \n",
       "0  Нападающий «Вашингтон Кэпиталз» Александр Овеч...  \n",
       "1  Власти Мексики объявили подделкой статую майя,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считываю данные\n",
    "df_train = pd.read_csv('data/news_train.txt',sep='\\t', header = None)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции очистки - от стоп-слов, лемматизация\n",
    "from nltk.tokenize import sent_tokenize\n",
    "rus_spacy = spacy.load('ru_core_news_sm')\n",
    "rus_stop = set(nltk.corpus.stopwords.words('russian'))\n",
    "\n",
    "def clean_tokens(text):\n",
    "    tokens_spacy = rus_spacy(text)\n",
    "    result = [str(token.lemma_).lower() for token in tokens_spacy if (str(token.lemma_).isalpha() or str(token.lemma_).isdigit()) and len(str(token.lemma_)) > 2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
       "      <td>[Нападающий, «, Вашингтон, Кэпиталз, », Алекса...</td>\n",
       "      <td>[нападающий, вашингтон, кэпиталз, александр, о...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1  \\\n",
       "0  sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  Нападающий «Вашингтон Кэпиталз» Александр Овеч...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Нападающий, «, Вашингтон, Кэпиталз, », Алекса...   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [нападающий, вашингтон, кэпиталз, александр, о...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tokens'] = df_train[2].apply(lambda x: tokenizers.simple_word_tokenize(x))\n",
    "df_train['clean_tokens'] = df_train[2].apply(lambda x: clean_tokens(x))\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28314388 -0.28777868 -2.2720418  -0.63242996  1.3500015   0.6867338\n",
      " -0.3596796   0.9810267  -1.3352921  -0.07510228 -1.5469109   2.0361385\n",
      " -0.20431805 -1.2667714   1.3965303   0.8531669   0.06001989 -0.3522906\n",
      " -1.4320478   1.2419431  -0.62719357  1.0496353   0.36072296  1.7299687\n",
      " -2.0626934   1.0852859   0.77997047  2.580381    0.6262289  -0.39766702\n",
      " -2.4815822   0.06595197 -2.7736921   1.8702773  -0.58519566 -0.25563875\n",
      " -0.08145706 -0.7377388   0.9655309  -0.41100553  1.5970007   0.943053\n",
      " -0.1442502  -0.9689439  -0.10089649 -0.87093085 -0.97564167  0.6204158\n",
      "  1.4662004   0.70554346 -0.1098365   0.4951234   0.6121644   1.5933162\n",
      " -1.0363703   0.35330403 -0.08810065 -0.8030877  -0.41920248 -1.6170603\n",
      "  0.49222204 -1.1579392  -0.9281027   0.96192735 -0.22509286  1.6618536\n",
      "  2.671127   -0.54887605 -0.74107426  0.42646894 -0.17938142 -2.12402\n",
      " -0.4228188  -1.2978323  -0.4956526   0.21263014 -0.36359313  0.2056176\n",
      " -0.7700504  -1.2205964   2.1848676  -0.6516354  -0.42598793 -0.44687098\n",
      "  1.1104913   0.86866766  0.6494141   0.96708757  0.43532526  0.1641893\n",
      "  0.34366435  1.0319839   0.3608098  -0.84822786 -0.54855704  1.0659647\n",
      "  0.53354704 -0.45809466 -0.36784065 -0.97197914]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elizaveta\\Classifications_2021\\venv\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# обучение Word2Vec\n",
    "model = gensim.models.Word2Vec(sentences=df_train.clean_tokens.values.tolist(), min_count=1, workers=8)\n",
    "\n",
    "print(model['нападающий'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сфабрикованные', 0.8191118240356445),\n",
       " ('chan', 0.8186121582984924),\n",
       " ('краденый', 0.81550532579422),\n",
       " ('удерзо', 0.8131376504898071),\n",
       " ('подлинность', 0.8102996945381165),\n",
       " ('гудзонский', 0.8078938126564026),\n",
       " ('невиновный', 0.8052371144294739),\n",
       " ('свидетельский', 0.8042401671409607),\n",
       " ('побуждение', 0.8000742197036743),\n",
       " ('грант', 0.7975072860717773)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# семантические ассоциации\n",
    "model.wv.similar_by_word('подделка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('полузащитник', 0.9831914305686951),\n",
       " ('форвард', 0.9745373725891113),\n",
       " ('голкипер', 0.9693038463592529),\n",
       " ('вратарь', 0.9632286429405212),\n",
       " ('анжи', 0.953568160533905),\n",
       " ('челси', 0.9504212737083435),\n",
       " ('реал', 0.9488823413848877),\n",
       " ('бавария', 0.9443444609642029),\n",
       " ('радулов', 0.941689670085907),\n",
       " ('ска', 0.9405420422554016)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('нападающий')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/news_test.txt',sep='\\t', header = None)\n",
    "df_test['clean_tokens'] = df_test[2].apply(lambda x: clean_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "themes = list(df_train[0].unique())\n",
    "all_words = list(set(model.wv.vocab.keys()))\n",
    "themes_from_df = df_train[0].values.tolist()\n",
    "words_from_df = df_train.clean_tokens.values.tolist()\n",
    "\n",
    "def prob_theme(themes):\n",
    "    res_list = dict()\n",
    "    for theme in themes:\n",
    "        res_list[theme] = 0\n",
    "    kol = 0\n",
    "    for all_theme in themes_from_df:\n",
    "        res_list[all_theme] += 1\n",
    "        kol += 1\n",
    "    for i in themes:\n",
    "        res_list[i] /= kol\n",
    "    return res_list\n",
    "\n",
    "def prob_words(words, themes):\n",
    "    res_list = dict()\n",
    "    for theme in themes:\n",
    "        for word in words:\n",
    "            res_list[(theme, word)] = 0\n",
    "    dict_th = dict()\n",
    "    for i in themes:\n",
    "        dict_th[i] = 0\n",
    "    for i in range(len(words_from_df)):\n",
    "        for j in words_from_df[i]:\n",
    "            res_list[(themes_from_df[i], j)] += 1\n",
    "            dict_th[themes_from_df[i]] += 1\n",
    "        \n",
    "    for label, feat in res_list:\n",
    "        res_list[label, feat] /= dict_th[label]\n",
    "    return res_list\n",
    "\n",
    "prob1 = prob_theme(themes)\n",
    "prob2 = prob_words(all_words, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0.8033333333333333\n"
     ]
    }
   ],
   "source": [
    "# Байес\n",
    "def klasif(sentence, classes, prob):\n",
    "    return max(classes.keys(), key = lambda cl: -log(classes[cl]) + sum(-log(prob.get((cl,feat), 0) + 10**(-25), 10**(-25)) for feat in sentence))\n",
    "\n",
    "themes_from_df = df_test[0].values.tolist()\n",
    "words_from_df = df_test.clean_tokens.values.tolist()\n",
    "res_test = []\n",
    "for i in range(len(words_from_df)):\n",
    "    res_test.append([klasif(words_from_df[i], prob1, prob2), themes_from_df[i]])\n",
    "right_count = 0\n",
    "for i in res_test:\n",
    "    if (i[0] == i[1]):\n",
    "        right_count += 1\n",
    "print(\"Test\", right_count / len(res_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.77\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "num_vocab = model.wv.vocab.copy()\n",
    "for i, key in enumerate(num_vocab):\n",
    "    num_vocab[key] = i\n",
    "num_vocab_themes = dict()\n",
    "for i, k in enumerate(themes):\n",
    "    num_vocab_themes[k] = i\n",
    "\n",
    "def to_vector(sentence):\n",
    "    res = [0] * len(num_vocab)\n",
    "    for i in sentence:\n",
    "        if (num_vocab.get(i,0)) != 0:\n",
    "            res[num_vocab[i]]+= 1\n",
    "    new_res = []\n",
    "    vse = 0\n",
    "    summa = 0\n",
    "    for i in res:\n",
    "        vse += 1\n",
    "        summa += i\n",
    "        if (vse == 50):\n",
    "            vse = 0\n",
    "            new_res.append(summa)\n",
    "            summa = 0\n",
    "    v = np.array(new_res)\n",
    "    normalized_v = v/(np.linalg.norm(v) + 0.000001)\n",
    "    return normalized_v.tolist()\n",
    "\n",
    "X_vec = []\n",
    "for i in df_train.clean_tokens.values:\n",
    "    X_vec.append(to_vector(i))\n",
    "Y_vec = []\n",
    "for i in df_train[0].values:\n",
    "    Y_vec.append(num_vocab_themes[i])\n",
    "\n",
    "my_svm = svm.SVC(kernel='rbf', gamma=5.)\n",
    "my_svm.fit(X_vec, Y_vec)\n",
    "my_svm.predict([X_vec[0]])\n",
    "\n",
    "k = 0\n",
    "for i in range(len(df_test[0].values)):\n",
    "    if (num_vocab_themes[df_test[0].values[i]] == my_svm.predict([to_vector(df_test.clean_tokens.values[i])])[0]):\n",
    "        k += 1\n",
    "print('Test:', k / len(df_test[0].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8393333333333334\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model_bert = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "def em_bert(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    emb = model_output.last_hidden_state[:, 0, :]\n",
    "    emb = torch.nn.functional.normalize(emb)\n",
    "    return emb[0].cpu().numpy()\n",
    "\n",
    "\n",
    "X_train_rub = []\n",
    "for i in df_train[2].values:\n",
    "    X_train_rub.append(em_bert(i, model_bert, tokenizer))\n",
    "Y_train_rub = []\n",
    "for i in df_train[0].values:\n",
    "    Y_train_rub.append(num_vocab_themes[i])\n",
    "\n",
    "X_test_rub = []\n",
    "for i in df_test[2].values:\n",
    "    X_test_rub.append(em_bert(i, model_bert, tokenizer))\n",
    "Y_test_rub = []\n",
    "for i in df_test[0].values:\n",
    "    Y_test_rub.append(num_vocab_themes[i])\n",
    "\n",
    "svm_rub = svm.SVC(kernel='rbf', gamma=5.)\n",
    "svm_rub.fit(X_train_rub, Y_train_rub)\n",
    "a_rubert = svm_rub.predict([X_train_rub[0]])\n",
    "\n",
    "k = 0\n",
    "for i in range(len(X_test_rub)):\n",
    "    if (Y_test_rub[i] == svm_rub.predict([X_test_rub[i]])[0]):\n",
    "        k += 1\n",
    "print(k / len(df_test[0].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
